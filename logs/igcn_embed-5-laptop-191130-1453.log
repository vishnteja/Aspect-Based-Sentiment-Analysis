cuda memory allocated: 2681344
n_trainable_params: 321903, n_nontrainable_params: 347432
> training arguments:
>>> model_name: igcn_embed
>>> model_ver: 5
>>> dataset: laptop
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x000001C256953EA0>
>>> learning_rate: 0.0003
>>> momentum: 0.95
>>> dropout: 0.5
>>> l2reg: 5e-05
>>> num_epoch: 50
>>> batch_size: 32
>>> log_step: 5
>>> embed_dim: 136
>>> hidden_dim: 300
>>> bert_dim: 768
>>> pretrained_bert_name: bert-base-uncased
>>> embeddings: d
>>> max_seq_len: 180
>>> polarities_dim: 3
>>> hops: 3
>>> device: cuda
>>> seed: 0
>>> valset_ratio: 0
>>> num_layers: 1
>>> in_channels: 100
>>> out_channels: 1
>>> downbot: 20
>>> kernel_size: 3
>>> position_dim: 100
>>> gating: GTU
>>> eval: False
>>> stanford_pos_model: C:/NLP_Programs/stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger
>>> stanford_pos_jar: C:/NLP_Programs/stanford-postagger-2018-10-16/stanford-postagger.jar
>>> glove_path: ./embeddings/glove.840B.300d.txt
>>> embedding_matrix_path: gen_data/embeddings/100_laptop_domain_embedding_matrix.dat
>>> model_class: <class 'models.igcn.IGCN_5'>
>>> dataset_file: {'train': './datasets/semeval14/Laptops_Train.xml.seg', 'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'}
>>> inputs_cols: ['text_raw_indices', 'aspect_indices', 'pos_indices', 'aspect_pos_indices', 'position_indices']
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.2319, acc: 0.3187
loss: 1.2011, acc: 0.3375
loss: 1.2133, acc: 0.3521
loss: 1.1719, acc: 0.3875
loss: 1.1772, acc: 0.3850
loss: 1.1778, acc: 0.3844
loss: 1.1633, acc: 0.3937
loss: 1.1610, acc: 0.3937
loss: 1.1487, acc: 0.4000
loss: 1.1362, acc: 0.4106
loss: 1.1288, acc: 0.4153
loss: 1.1201, acc: 0.4208
loss: 1.1126, acc: 0.4255
loss: 1.1060, acc: 0.4295
> val_acc: 0.5878, val_f1: 0.4135
> best_val_acc: 0.0000, best_val_f1: 0.0000
>> saved: state_dict/igcn_embed_5_laptop_val_acc0.5878
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 1.0496, acc: 0.5312
loss: 1.0159, acc: 0.5402
